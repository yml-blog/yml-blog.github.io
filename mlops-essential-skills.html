<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MLOps: A Must-Have Skill for Efficient Model Management and Deployment</title>
    <link href="favicon.png" rel="icon">
    <link href="https://fonts.googleapis.com/css?family=Hind:300,400,500,600,700" rel="stylesheet">
    <link href="css/font-awesome.min.css" rel="stylesheet">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style_blog.css" rel="stylesheet">
    <link href="css/blog_post.css" rel="stylesheet">
    <link href="css/nav.css" rel="stylesheet">
    <link href="css/prism.css" rel="stylesheet">
    <script src="js/loadHeader.js"></script>
</head>
<body>
    <div class="main-content">
        <div class="container">
            <header class="header">
                <div class="content text-center">
                    <h1>MLOps: A Must-Have Skill for Efficient Model Management and Deployment</h1>
                    <p class="lead">By Yangming Li</p>
                </div>
            </header>
            
            <div class="article-metadata">
                <span class="read-time"><i class="fa fa-clock-o"></i> 8 min read</span>
                <span class="word-count"><i class="fa fa-file-text-o"></i> Approx. 1000 words</span>
                <div class="keywords">
                    <strong>Keywords:</strong> MLOps, Machine Learning, Model Deployment, MLflow, Weights & Biases, DevOps
                </div>
            </div>

            <main>
                <section class="intro">
                    <p>As machine learning (ML) technologies continue to mature, a central concern for many organizations is how to deploy models to production efficiently and ensure they are maintained seamlessly. This is where MLOps, or Machine Learning Operations, comes into play. MLOps combines the principles of traditional DevOps with the unique needs of machine learning, aiming to improve the efficiency of model development, deployment, and monitoring through automation and collaboration.</p>
                </section>

                <section>
                    <h2>What is MLOps?</h2>
                    <p>MLOps is a systematic framework that covers a series of steps from data management and model development to deployment and continuous monitoring. The goal is to accelerate model deployment through automated, standardized processes, ensuring that models perform consistently in production.</p>
                    
                    <div class="technical-box">
                        <h3>Key Aspects of MLOps</h3>
                        <ul>
                            <li><strong>Data Management:</strong> Ensures data version control and consistency</li>
                            <li><strong>Model Training and Evaluation:</strong> Supports automated model selection and performance tuning</li>
                            <li><strong>Model Deployment:</strong> Automates model deployment through CI/CD pipelines</li>
                            <li><strong>Model Monitoring:</strong> Continuously tracks model performance to detect issues like model drift</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Why is MLOps Important?</h2>
                    <ul>
                        <li><strong>Accelerates Model Deployment:</strong> MLOps significantly reduces the time from model development to deployment, enabling organizations to respond quickly to market changes.</li>
                        <li><strong>Improves Collaboration:</strong> A unified platform allows data scientists and development teams to collaborate more effectively, reducing duplicated efforts.</li>
                        <li><strong>Continuous Monitoring and Improvement:</strong> MLOps enables models to be automatically monitored post-deployment. When model performance degrades, it can trigger retraining, ensuring the model consistently performs at its best.</li>
                    </ul>
                </section>

                <section>
                    <h2>MLOps Example: Building a Simple ML Pipeline</h2>
                    <p>Below is an example of how to create and manage an ML pipeline using both MLflow and Weights & Biases (W&B), two popular tools in MLOps. MLflow is used for tracking and managing model versions and deployments, while Weights & Biases helps visualize and monitor metrics in real time.</p>

                    <div class="code-block">
                        <pre><code class="language-python">
import mlflow
import mlflow.sklearn
import wandb  # Weights & Biases library
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Initialize Weights & Biases project
wandb.init(project="mlops_example", name="random_forest_run")

# Load the dataset
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(
    iris.data, iris.target, test_size=0.2, random_state=42
)

# Start tracking the experiment with MLflow
with mlflow.start_run():
    # Train the Random Forest model
    clf = RandomForestClassifier(n_estimators=100, random_state=42)
    clf.fit(X_train, y_train)

    # Predict and evaluate the model
    predictions = clf.predict(X_test)
    acc = accuracy_score(y_test, predictions)
    print(f"Model Accuracy: {acc}")

    # Log the model and metrics with MLflow
    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(clf, "random_forest_model")

    # Log metrics and parameters with Weights & Biases
    wandb.log({"accuracy": acc, "n_estimators": 100})
                        </code></pre>
                    </div>

                    <div class="technical-box">
                        <h3>Code Explanation</h3>
                        <ul>
                            <li><strong>MLflow:</strong> Manages the ML experiment and model deployment, allowing you to track performance across experiments and deploy the best-performing model.</li>
                            <li><strong>Weights & Biases (W&B):</strong> Logs metrics like accuracy and parameters in real time, enabling interactive visualizations of model performance and hyperparameter tuning.</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Key MLOps Tools</h2>
                    <div class="technical-box">
                        <ul>
                            <li><strong>MLflow:</strong> An open-source platform for managing the ML lifecycle, including experiment tracking, model management, and deployment.</li>
                            <li><strong>Weights & Biases:</strong> A powerful tool for tracking, visualizing, and collaborating on ML experiments. It provides interactive dashboards and helps data scientists monitor training metrics and hyperparameters.</li>
                            <li><strong>Kedro:</strong> A data science project management framework that helps build modular and reproducible ML code.</li>
                            <li><strong>Kubeflow:</strong> Automates ML workflows on Kubernetes, supporting the entire lifecycle from model training to deployment.</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>MLOps Reference Architecture</h2>
                    <p>A reference architecture serves as a blueprint for designing ML systems, incorporating industry best practices and proven patterns. By following a reference architecture, organizations can build scalable, maintainable ML solutions using consistent and repeatable approaches.</p>
                    
                    <div class="technical-box">
                        <h3>Understanding Reference Architecture</h3>
                        <p>A reference architecture is more than just a diagram - it's a comprehensive blueprint used to design IT solutions, particularly ML systems. It provides structured approaches for integrating various IT elements and patterns commonly used in solution design. By adopting such architecture, organizations can:</p>
                        <ul>
                            <li>Leverage industry best practices</li>
                            <li>Streamline development processes</li>
                            <li>Minimize technical debt</li>
                            <li>Ensure scalability and maintainability</li>
                        </ul>
                    </div>
                    
                    <img src="img/MLOPs articture.png" alt="MLOps Reference Architecture" class="img-fluid technical-diagram">
                    
                    <div class="technical-box">
                        <h3>Detailed Component Breakdown</h3>
                        
                        <h4>1. Development Environment</h4>
                        <p>The journey begins in the experiment/development environment where data scientists execute orchestrated ML experiments. This environment includes:</p>
                        <ul>
                            <li>Orchestrated ML experiments for systematic model development</li>
                            <li>Source code management and version control</li>
                            <li>Integration with development tools and notebooks</li>
                        </ul>

                        <h4>2. CI/CD Pipeline Implementation</h4>
                        <p>The CI/CD pipeline forms the backbone of automated MLOps:</p>
                        <ul>
                            <li><strong>Continuous Integration (CI):</strong>
                                <ul>
                                    <li>Automated code testing and validation</li>
                                    <li>Creation of deployable artifacts</li>
                                    <li>Package and executable generation</li>
                                </ul>
                            </li>
                            <li><strong>Continuous Delivery (CD):</strong>
                                <ul>
                                    <li>Automated deployment to production</li>
                                    <li>Pipeline deployment automation</li>
                                    <li>Model deployment orchestration</li>
                                </ul>
                            </li>
                        </ul>

                        <h4>3. Feature Store</h4>
                        <p>The feature store is a critical component that:</p>
                        <ul>
                            <li>Provides consistent feature serving across development and production</li>
                            <li>Enables feature reuse and reproducibility</li>
                            <li>Maintains feature versioning and compatibility</li>
                            <li>Feeds data to both development experiments and production services</li>
                        </ul>

                        <h4>4. Model Management</h4>
                        <p>Comprehensive model management includes:</p>
                        <ul>
                            <li><strong>Metadata Store:</strong>
                                <ul>
                                    <li>Records pipeline execution logs</li>
                                    <li>Stores training artifacts and hyperparameters</li>
                                    <li>Maintains experiment tracking information</li>
                                </ul>
                            </li>
                            <li><strong>Model Registry:</strong>
                                <ul>
                                    <li>Centralized model version control</li>
                                    <li>Model artifact storage and management</li>
                                    <li>Model lineage tracking</li>
                                </ul>
                            </li>
                        </ul>

                        <h4>5. Automated Operations</h4>
                        <p>The automation aspect includes:</p>
                        <ul>
                            <li><strong>Continuous Monitoring:</strong>
                                <ul>
                                    <li>Real-time performance tracking</li>
                                    <li>Statistical analysis of predictions</li>
                                    <li>Data drift detection</li>
                                </ul>
                            </li>
                            <li><strong>Automated Triggers:</strong>
                                <ul>
                                    <li>Performance-based pipeline activation</li>
                                    <li>Scheduled retraining mechanisms</li>
                                    <li>Data-driven trigger systems</li>
                                </ul>
                            </li>
                            <li><strong>Model Retraining:</strong>
                                <ul>
                                    <li>Automated training pipeline execution</li>
                                    <li>Fallback mechanism implementation</li>
                                    <li>Backup model management</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="info-box">
                        <h3>Implementation Considerations</h3>
                        <p>When implementing this reference architecture, organizations should:</p>
                        <ul>
                            <li>Start with essential components and gradually expand</li>
                            <li>Ensure robust testing at each automation stage</li>
                            <li>Maintain clear documentation and monitoring practices</li>
                            <li>Consider scalability requirements from the beginning</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Automated Experiment Tracking in MLOps</h2>
                    <p>Machine Learning is inherently experimental in nature, with data scientists and ML engineers constantly tweaking various aspects of their ML pipelines to find optimal solutions. This experimental nature creates a need for robust tracking systems.</p>

                    <div class="technical-box">
                        <h3>Why Automated Experiment Tracking?</h3>
                        <p>Manual tracking becomes impractical due to:</p>
                        <ul>
                            <li>Multiple data transformations and algorithms to test</li>
                            <li>Various model evaluation metrics</li>
                            <li>Different hyperparameter combinations</li>
                            <li>Need for reproducibility and transparency</li>
                        </ul>
                    </div>

                    <div class="technical-box">
                        <h3>What Should Be Tracked?</h3>
                        <ul>
                            <li><strong>Code and Configuration:</strong>
                                <ul>
                                    <li>Experiment generation code</li>
                                    <li>Environment configuration files</li>
                                    <li>Runtime specifications</li>
                                </ul>
                            </li>
                            <li><strong>Data Specifications:</strong>
                                <ul>
                                    <li>Data sources and types</li>
                                    <li>Data formats</li>
                                    <li>Cleaning procedures</li>
                                    <li>Augmentation methods</li>
                                </ul>
                            </li>
                            <li><strong>Model Information:</strong>
                                <ul>
                                    <li>Model parameters</li>
                                    <li>Hyperparameters</li>
                                    <li>Evaluation metrics</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="info-box">
                        <h3>Benefits of Automated Tracking</h3>
                        <ul>
                            <li><strong>Reproducibility:</strong> Ensures experiments can be replicated accurately</li>
                            <li><strong>Performance Monitoring:</strong> Enables systematic tracking of system performance</li>
                            <li><strong>Result Comparison:</strong> Facilitates easy comparison between different model versions</li>
                            <li><strong>Decision Support:</strong> Provides data-driven insights for improvement</li>
                            <li><strong>Transparency:</strong> Allows others to understand and validate the process</li>
                        </ul>
                    </div>

                    <div class="technical-box">
                        <h3>Modern Experiment Tracking Systems</h3>
                        <p>Contemporary tracking systems offer:</p>
                        <ul>
                            <li>Interactive dashboards for visualization</li>
                            <li>Programmatic access to tracking data</li>
                            <li>Model registry integration</li>
                            <li>Automated metadata organization</li>
                            <li>Run comparison capabilities</li>
                        </ul>
                    </div>

                    <div class="market-box">
                        <h3>Market Solutions</h3>
                        <p>While custom tracking solutions can be developed, several market options exist:</p>
                        <ul>
                            <li><strong>Open-source Solutions:</strong>
                                <ul>
                                    <li>MLflow</li>
                                    <li>DVC (Data Version Control)</li>
                                    <li>Sacred</li>
                                </ul>
                            </li>
                            <li><strong>Commercial Solutions:</strong>
                                <ul>
                                    <li>Weights & Biases</li>
                                    <li>Neptune.ai</li>
                                    <li>Comet.ml</li>
                                </ul>
                            </li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>The Model Registry: Managing ML Model Lifecycle</h2>
                    <p>The model registry is a crucial component in MLOps architecture that serves as a centralized repository for managing the entire lifecycle of machine learning models, from creation to archiving.</p>

                    <img src="img/model registry.png" alt="Model Registry Architecture" class="technical-diagram">

                    <div class="technical-box">
                        <h3>Understanding Model Lifecycle Management</h3>
                        <p>Models transition through different environments in their lifecycle:</p>
                        <ul>
                            <li><strong>Development:</strong> Where models are created and initially tested</li>
                            <li><strong>Staging:</strong> Where models undergo validation and integration testing</li>
                            <li><strong>Production:</strong> Where models are deployed for actual use</li>
                        </ul>
                    </div>

                    <div class="warning-box">
                        <h3>Moving Beyond "Over the Fence" Deployment</h3>
                        <p>Traditional model deployment often involves a data scientist simply "throwing the model over the fence" to operations teams through:</p>
                        <ul>
                            <li>Email attachments</li>
                            <li>USB drives</li>
                            <li>Shared network drives</li>
                        </ul>
                        <p>This approach lacks proper versioning, tracking, and automation capabilities.</p>
                    </div>

                    <div class="technical-box">
                        <h3>Core Functions of the Model Registry</h3>
                        <ul>
                            <li><strong>Centralized Storage:</strong>
                                <ul>
                                    <li>Model artifacts</li>
                                    <li>Deployment configurations</li>
                                    <li>Model dependencies</li>
                                </ul>
                            </li>
                            <li><strong>Lifecycle Management:</strong>
                                <ul>
                                    <li>Version control</li>
                                    <li>Environment transitions</li>
                                    <li>Deployment history</li>
                                    <li>Model archiving</li>
                                </ul>
                            </li>
                            <li><strong>Integration Capabilities:</strong>
                                <ul>
                                    <li>CI/CD workflow integration</li>
                                    <li>Automated testing triggers</li>
                                    <li>Deployment automation</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="process-box">
                        <h3>Model Registration Process</h3>
                        <ol>
                            <li><strong>Experimentation Phase:</strong>
                                <ul>
                                    <li>Multiple iterations tracked by experiment tracking system</li>
                                    <li>Metadata stored in metadata store</li>
                                </ul>
                            </li>
                            <li><strong>Model Selection:</strong>
                                <ul>
                                    <li>Best performing model identified</li>
                                    <li>Model promoted to registry</li>
                                </ul>
                            </li>
                            <li><strong>Validation:</strong>
                                <ul>
                                    <li>Automated testing in staging environment</li>
                                    <li>Integration verification</li>
                                </ul>
                            </li>
                            <li><strong>Deployment:</strong>
                                <ul>
                                    <li>CD pipeline triggered</li>
                                    <li>Prediction services updated</li>
                                </ul>
                            </li>
                            <li><strong>Archival:</strong>
                                <ul>
                                    <li>Previous model decommissioned</li>
                                    <li>Model history maintained</li>
                                </ul>
                            </li>
                        </ol>
                    </div>

                    <div class="info-box">
                        <h3>Design Flexibility</h3>
                        <p>The model registry can be implemented in different ways:</p>
                        <ul>
                            <li><strong>Separated System:</strong> Model registry focuses solely on model storage and lifecycle management, while experiment tracking and metadata storage are handled separately.</li>
                            <li><strong>Unified System:</strong> Model registry includes experiment tracking and metadata storage capabilities in a single platform.</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>Feature Store: The Cornerstone of Enterprise ML</h2>
                    <p>A feature store serves as a centralized repository for standardizing the definition, storage, and access of features across all stages of the ML lifecycle - from experimentation to production serving.</p>

                    <img src="img/feature_store.png" alt="Feature Store Architecture" class="technical-diagram">

                    <div class="technical-box">
                        <h3>Understanding Feature Engineering</h3>
                        <p>Feature engineering involves transforming raw data into meaningful inputs for ML algorithms through:</p>
                        <ul>
                            <li>Numerical transformations (standardization, normalization)</li>
                            <li>Categorical variable encoding</li>
                            <li>Domain-specific feature creation</li>
                            <li>Value grouping and aggregation</li>
                        </ul>
                    </div>

                    <div class="warning-box">
                        <h3>Enterprise Challenges Without Feature Store</h3>
                        <p>Organizations often face these issues:</p>
                        <ul>
                            <li>Duplicate feature engineering efforts across teams</li>
                            <li>Inconsistent feature definitions</li>
                            <li>Redundant storage infrastructure</li>
                            <li>Limited feature discovery and reuse</li>
                            <li>Data skew between training and serving</li>
                        </ul>
                    </div>

                    <div class="technical-box">
                        <h3>Feature Store Capabilities</h3>
                        <ul>
                            <li><strong>Data Integration:</strong>
                                <ul>
                                    <li>Ingests raw data from multiple sources</li>
                                    <li>Handles both batch and streaming data</li>
                                    <li>Standardizes transformation processes</li>
                                </ul>
                            </li>
                            <li><strong>Storage and Serving:</strong>
                                <ul>
                                    <li>Centralized feature storage</li>
                                    <li>High-throughput batch serving</li>
                                    <li>Low-latency real-time serving</li>
                                    <li>API access for various use cases</li>
                                </ul>
                            </li>
                            <li><strong>Metadata Management:</strong>
                                <ul>
                                    <li>Feature versioning</li>
                                    <li>Data lineage tracking</li>
                                    <li>Feature documentation</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="benefits-box">
                        <h3>Key Benefits</h3>
                        <ul>
                            <li><strong>Accelerated Experimentation:</strong>
                                <ul>
                                    <li>Quick access to curated feature sets</li>
                                    <li>Feature discovery and reuse</li>
                                    <li>Consistent feature definitions</li>
                                </ul>
                            </li>
                            <li><strong>Continuous Training:</strong>
                                <ul>
                                    <li>Automated feature updates</li>
                                    <li>Consistent training datasets</li>
                                    <li>Version-controlled features</li>
                                </ul>
                            </li>
                            <li><strong>Production Serving:</strong>
                                <ul>
                                    <li>Batch and real-time prediction support</li>
                                    <li>Environment symmetry</li>
                                    <li>Prevention of data skew</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="info-box">
                        <h3>Environment Symmetry</h3>
                        <p>The feature store ensures environment symmetry by:</p>
                        <ul>
                            <li>Providing identical feature computation across all environments</li>
                            <li>Maintaining consistent feature definitions from development to production</li>
                            <li>Preventing data skew between training and serving</li>
                            <li>Centralizing feature access for all ML pipeline stages</li>
                        </ul>
                    </div>
                </section>

                <section>
                    <h2>The Metadata Store: Enabling Automated MLOps Workflows</h2>
                    <p>The metadata store serves as a centralized repository for managing all information about artifacts created during the execution of ML pipelines, playing a crucial role in enabling fully automated MLOps workflows.</p>

                    <img src="img/meta_database.png" alt="Metadata Store Architecture" class="technical-diagram">

                    <div class="technical-box">
                        <h3>Understanding MLOps Metadata</h3>
                        <p>Metadata includes information about:</p>
                        <ul>
                            <li>Data source versions and modifications</li>
                            <li>Model hyperparameters and versions</li>
                            <li>Training evaluation results</li>
                            <li>Pipeline execution logs</li>
                            <li>Hardware utilization metrics</li>
                        </ul>
                    </div>

                    <div class="info-box">
                        <h3>Key Aspects of ML Metadata</h3>
                        <div class="aspect-grid">
                            <div class="aspect">
                                <h4>Model Drift Detection and Automated Response</h4>
                                <img src="img/model_drift.png" alt="Model Drift Detection" class="aspect-image">
                                <p class="detailed-explanation">
                                    The metadata store enables automatic monitoring of MLOps pipelines and facilitates automated incident response. When the system detects model drift, it can automatically trigger model retraining. Continuous monitoring of evaluation metrics allows the system to detect performance decay, prompting automated responses:
                                </p>
                                <ul class="drift-response">
                                    <li>Automatic detection of model drift through continuous metric monitoring</li>
                                    <li>Automated model retraining when performance decay is detected</li>
                                    <li>Automated rollback capabilities to previous stable versions</li>
                                    <li>System continuity maintenance during root cause analysis</li>
                                </ul>
                            </div>
                            <div class="aspect">
                                <h4>Monitoring Tools</h4>
                                <img src="img/meta_data_monitor_tool.png" alt="Monitoring Dashboard" class="aspect-image">
                                <p>Enables tracking of pipeline execution status and system health</p>
                            </div>
                        </div>
                    </div>

                    <div class="technical-box">
                        <h3>Metadata Store Functions</h3>
                        <ul>
                            <li><strong>Centralized Management:</strong>
                                <ul>
                                    <li>Experiment logs</li>
                                    <li>Artifact metadata</li>
                                    <li>Model information</li>
                                    <li>Pipeline execution data</li>
                                </ul>
                            </li>
                            <li><strong>Integration Capabilities:</strong>
                                <ul>
                                    <li>User interface for metadata access</li>
                                    <li>API for automated logging</li>
                                    <li>Pipeline component interaction</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="benefits-box">
                        <h3>Automation Benefits</h3>
                        <ul>
                            <li><strong>Automated Monitoring:</strong>
                                <ul>
                                    <li>Continuous performance tracking</li>
                                    <li>Automatic incident response</li>
                                    <li>Model drift detection</li>
                                </ul>
                            </li>
                            <li><strong>Automated Recovery:</strong>
                                <ul>
                                    <li>Automated model retraining triggers</li>
                                    <li>Automatic rollback capabilities</li>
                                    <li>System resilience</li>
                                </ul>
                            </li>
                        </ul>
                    </div>

                    <div class="info-box">
                        <h3>Reproducibility and Trust</h3>
                        <p>The metadata store enables:</p>
                        <ul>
                            <li>Experiment reproduction for validation</li>
                            <li>Scientific rigor in ML processes</li>
                            <li>Transparent decision tracking</li>
                            <li>Root cause analysis capabilities</li>
                        </ul>
                    </div>
                </section>

                <section class="conclusion">
                    <h2>Conclusion</h2>
                    <p>The introduction of MLOps greatly improves the model management process in production environments. By automating deployment and continuous monitoring, MLOps ensures model stability and performance. This not only enhances team collaboration but also ensures that models continuously meet business needs. As more companies adopt MLOps, the future of ML development will become more efficient and automated.</p>
                    <p>Using tools like MLflow and Weights & Biases together provides comprehensive tracking, management, and visualization, making MLOps workflows robust and adaptable for scalable ML solutions.</p>
                </section>
            </main>
            
            <footer class="footer text-center">
                <p>&copy; 2024 Yangming Li. All rights reserved.</p>
            </footer>
        </div>
    </div>

    <!-- Scripts -->
    <script src="js/jquery-2.1.4.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/scripts.js"></script>
    <script src="js/prism.js"></script>
    <script src="js/blogPanel.js"></script>
</body>
</html> 